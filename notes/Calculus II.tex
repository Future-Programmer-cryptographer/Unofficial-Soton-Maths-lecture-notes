\title{MATH1065: Calculus II}
\author{Shub Das \\ sd2g25@soton.ac.uk}
\date{\today}

\maketitle

\tableofcontents 

\part{Ordinary Differential Equations}

\chapter{First-Order ODE}

When dealing with ODEs \footnote{Get used to seeing them because they will appear everywhere}, there are a few main ways of solving them.  

I won't be including examples in most of the sections, mostly because well, just do a bunch of questions to get better at ODEs. 

\section{Separation of Variables}

This is when the ODE is in terms of $x$ and $y$ so that we can separate the variables 

$$
g(y) \frac{dy}{dx} = h(x) \implies \int g(y) \ dy = \int h(x) \ dx + c 
$$

\section{Integrating Factor}

This method is used to solve Linear 1st order ODEs. Say we have an ODE in the form of:
$$
\frac{dy}{dx} + p(x) y = q(x) 
$$

The key thing to note here is that $y$ is linear, and the coefficients on depend on $x$ 

When we look at the ODE above, we might wish that it were the derivative of a product because: 
$$
\frac{d}{dx} (\mu y) = \mu \frac{dy}{dx} + \mu y 
$$


And if we compare that with $\mu \frac{dy}{dx} + \mu p(x) \ y$ they only match if $\mu = \mu \ p(x)$


Therefore, we can introduce this term called the \textbf{integrating factor} so that we can make the DE separable and turn the LHS into a product derivative:
$$
\frac{d}{dx} [ y(x) e^{P(x})] = q(x) e^{\mu(x)} \text{ with } \mu(x) = \int p(x)\ dx 
$$
Then we multiply by the integrating factor (think back to converting it to a direct integration form). Note that the constant appears after we multiply by the integrating factor, not before. 
$$ 
y(x) = e^{-P(x)} \left[ \int q(x) e^{p(x)} \ dx + c] \right]
$$

\subsection{Note on integrating factor}

One might wonder why $\mu = e^{\int p(x) \ dx} $ is the integrating factor and nothing else. The short answer is that we need a function whose derivative is proportional to itself, something we need for the product rule to work. 

The goal we had in mind was to turn $\frac{d}{dx} + p(x) y$ into a derivative of a product $\frac{d}{dx} (\mu \ y)$ using the product rule

So when we try and solve $\frac{d\mu}{dx} = p(x) \ \mu$ after separating the variables and integrating, we get $\mu = e^{\int p(x) \ dx}$ and voila! We have the exponential. 

\section{Homogeneous ODEs}

A 1st order ODE is called \textbf{homogeneous}\footnote{If we're talking origins of words, homogeneous is Greek which means something of the same kind/type/origin. So for ODEs, it just means that equations treat the variables the same way under scaling, hence the ratio!} if it can be written in the form of: 
$$ 
\frac{dy}{dx}= F \left (\frac{y}{x} \right)
$$

i.e. the RHS depends only on the $y/x$ ratio.

Usually in questions, this form may not be obvious, and some rearrangement will be required to get it in the intended form, but once we have it, we can make the substitution: 

$$
z = \frac{y}{x} \implies y = z \ x 
$$

Then, we can use the product rule to differentiate and get: 
$$
\frac{dy}{dx} = z + x \ \frac{dz}{dx}
$$

And now we can substitute this back into the equation and rearrange to get: 
$$
z + x \ \frac{dz}{dx} = F(z)  \implies x \ \frac{dz}{dx} = F(z) - z
$$

We can now see that it's a separable ODE: 
$$
\frac{1}{F(z) - z} \ dz = \frac{1}{x} \ dx 
$$

\subsection{Note on the substitution}

The intuition behind the substitution was not covered in lectures, hence I endeavour to explain it in my notes. 

It mostly comes down to the fact that the homogeneous equation is treating the variables like a ratio, which isn't great to deal with it. So when we introduce another variable, we turn the equation into something we know how to differentiate using product rule. This will become more useful later when dealing with energy calculations later (hopefully!) as it makes separation of variables possible. 

Think of it as $u$-sub when dealing with integration, it's exactly the same thing. But more importantly, if scaling both variables changes nothing, the only thing that something depends on is $y/x$, hence the substitution. Think of it as describing the gradient of a line, we can do that with $m = y/x$ or $y = mx +c$ 

\chapter{Second Order ODE}


\section{2nd order ODEs with constant coefficients}

Okay, lots of words in that section title, and it's important we know what we mean when we talk about them. 

The term \textbf{order} is to do with the derivative. So 2nd order means the highest derivative appearing is $\frac{d^2y}{dx^2}$

This also means the solution space has two degrees of freedom or independent behaviours or constants 

\textbf{Linear} means the function $y^{\prime \prime}$ and its derivatives appear to the first \textbf{power}, they are not multiplied and the coefficients are independent of $y$. The term linear here is in fact linked to linear algebra and satisfies the axioms of linearity, i.e. if $y_1$ and $y_2$ are solutions, then so is $c_1y_1 + c_2 y_2$. So the solution here is a vector space. 

\textbf{Constant Coefficients} - all this means is that the numbers multiplying $y^{\prime \prime} y^{\prime}$ and $y$ do not depend on $x$. And this is also the reason why exponentials appear. 

As another note, \textbf{homogeneous} for a second order ODE just means that the RHS term is $0$, or in dynamics terms, there is no forcing term. 

\section{Method of solution} 

I won't go into detail about the cases or the method for solving DEs, as this is something  covered in the actual lecture notes. What I will talk about, however, is some of the intuition behind the method and the roots and the solutions. 

\subsection{Auxiliary equation}

This is also called the characteristic equation\footnote{I hope that rings a bell from eigenvalues and linear algebra!} 

Suppose we substitute $y = e^{\lambda x}$ into the 2nd order DE 
$$
a \frac{d^2y}{dx^2} + b \frac{dy}{dx} + cy = 0
$$

We get 
$$
(a\lambda^2 + b\lambda + c)e^{\lambda x} = 0
$$

And since $e^{\lambda x} \neq 0$, we are left with $a \lambda^2 + b\lambda + c = 0$ which you might notice is very similar to when hunting for eigenvalues from linear algebra. 

There's is a very good reason for why this is called the characteristic equation, and its because the roots characterise the behaviour of the system that is modelled by the ODE. 

\subsection{Roots} 

As I mentioned before, a 2nd order ODE has a 2D solution space, so as we found that the auxiliary equation is a quadratic, we get roots (counting multiplicity). Therefore, all solutions are just the linear combination of those roots, and we can be certain that nothing is missing from our solution space. 

This is also why if our discriminant of the quadratic is $0$, the second solution is multiplied by $x$ so that our solutions remain independent. In other words, $e^{\lambda x}$ has already used up one dimension, so $xe^{\lambda x}$ is the next simplest independent function\footnote{There is a power series justification for this, something which I won't be covering in my notes (yet)!}

\section{Euler Type equations}


Compared to the `usual' ODEs seen before, an Euler-type ODE has the form: 

$$
ax^2 \frac{d^2y}{dx^2} + bx \frac{dy}{dx} + cy = 0
$$

Now compare this to the constant-coefficient of $a\frac{d^2y}{dx^2} + b \frac{dy}{dx} + cy = 0$. 

Both are linear, second order, and homogeneous. But the key difference we can see is that the coefficients depend on $x$, but in a structured way, i.e., each derivative is multiplied by a power of $x$ that matches its order. This is where knowing terminology is important. 

It's also important to note that Euler type equations are scale-invariant. So if you rescale $x \rightarrow kx$, then every term scales the same way. 

\subsection{Substitutions}

Instead of just stating the substitution that is used to solve these kinds of ODEs, I think it's more helpful to think about what has worked before and why it may or may not work again. 

In constant-coefficients ODEs, the substitution $y = e^{\lambda x}$ works because its derivative reproduces the same kind of structure. But in this case, as the coefficients depend on $x$, the same kind of differentiation alone doesn't preserve that form. 

So instead, we ask ourselves a better questions, namely what functions behave better under scaling rather than translation. And the answer? $$y = x^{\lambda}$$

Now, we pause and ponder why this function works. 

These kind of equations are all about scaling and matching powers of $x$. Think about it this way: 
$$
y \sim x^{\lambda - 1}, \quad  y \sim x^{\lambda - 2}
$$

And if we multiply by the matching powers of $x$ we have: 

$$
x^2y \sim x^{\lambda}, \quad  xy \sim x^{\lambda}
$$

What we have done is just turned an ODE into an algebraic equation in $\lambda$ 


The entire point of substitutions is looking for patterns. For constant-coefficients equations, we look at the translation symmetry, and use the property of exponentials. And for Euler type, we look at the scaling symmetry, and make use of the power laws. 



\subsection{Change of variables}

There is more detail in the lecture notes about using the auxiliary equation, so I will leave that as an exercise to the reader and focus more on the intuition side of things when it comes to dealing with this seemingly mysterious but clever method of change of variables. 

Earlier with homogeneous equations, we introduced the substitution $z = \frac{y}{x}$ The motivation behind this was all to do with turning scaling into a translation, and in a similar vein, we use the same tactic. 

Bear in mind that scaling a variable in $x$ is translation in $\ln x$

$$
x \rightarrow kx \implies \ln (kx) = \ln x + \ln k
$$

And we already know how to solve translation-invariant equations, which are just constant-coefficient ODEs. 

So if roots are repeated on complex, we prefer to change variables before $x^{\lambda}$ won't give us two independent solutions. 

If we abstract things a bit more, the solutions to an ODE form a vector space, and the dimension is the just the order of the equation. And the entire change of variables is just a change of basis for the problem, so essentially, we are just changing basis to diagonalise the differential operator\footnote{More on eigenfunctions later}

\chapter{Wronskian}

\section{The joy of discovery}

Before I define anything or introduce new terms, let's start with something simple. I was particularly dissatisfied with this section of the module because I feel that mathematics should feel like a journey of discovery instead of a brick being slammed on the brain. So here is my attempt at `discovering' the Wronskian instead of telling you immediately what it is. 

Suppose we're solving a homogeneous linear 2nd order ODE. Immediately, we can see two things:

\begin{enumerate}
    \item The set of solutions is a linear combination 
    \item The solution space has dimension 2 
\end{enumerate}

This already sounds suspiciously like linear algebra, and that’s because it is. If we just take a step back from ODEs and just look at the operator, we can see that differentiation is linear, multiplication by functions is linear, and that addition is linear. So the moment we accept the linearity of differentiation, we can see that any linear combination of solution is also a solution. 

So now, the real problem is not `find solutions', but rather how do we know whether two solutions are genuinely different, or just the same solution in disguise?

If one solution is just a scalar multiple of the other, then we don’t have two degrees of freedom, we have only one, which is not enough 

\subsection{Linear independence}

Another point - what does `linearly independent' actually mean here?

For vectors, we have a good idea of how to check for independence, namely to use a matrix, compute its determinant, and if it's non-zero, we have independent vectors. 

But for functions, that doesn't seem obvious. Functions vary with variables, usually $x$ and their scalar multiples could also vary. Therefore, we are in need of a test. A test that checks whether two functions point in different directions or not. 


If two functions were multiples of each other, we might expect:
$$\frac{y_1(x)}{y_2(x)} = \text{constant}$$

But if we think about it, what if one function vanishes? Or the ratio behaves badly? Or if the dependence only shows up in derivatives? 

Looking at the points above, values alone are not enough, we need more information, like their derivatives.

\subsection{Determinant}

If two functions are really the same “direction”, then two things must be true: 
\begin{itemize}
    \item Their values are proportional 
    \item Their slopes are proportional
\end{itemize}

That’s the key idea.

So at a point $x$, we look at the vectors:
$$(y_1(x), y_1'(x)), \quad (y_2(x), y_2'(x))$$

And now we are back in linear algebra. If these two vectors point in the same direction, the area they span is zero.

And how do we measure area? With a determinant! 

Therefore, we consider: 
\begin{vmatrix}
y_1(x) & y_2(x) \\
y_1'(x) & y_2'(x)
\end{vmatrix}

And this new fancy thing, is what we call the \textbf{Wronskian}


\subsection{What the Wronskian actually measures}

The signed area spanned by the solution curves and their derivatives at a point. Thus, a zero area means collapse, indicating that the functions are dependent. Conversely, a non-zero area implies two genuine directions, signifying independence. 

So when we say:
$$W(x) \neq 0$$
what we really mean is that the solutions span the full solution space.

For solutions of a linear homogeneous ODE, the Wronskian either vanishes everywhere or nowhere (on an interval). There is no `sometimes independent' and therefore, checking the Wronskian at one point is enough.


At this point, we can see that: 
\begin{itemize}
    \item Solutions form a vector space
    \item Fundamental solutions form a basis
    \item The Wronskian is the determinant of that basis
    \item Initial conditions are just coordinates in that basis
\end{itemize}



\subsection{Fun generalisations}

For an nth-order ODE, we can define: 

$$W(y_1,\dots,y_n)(x) =
\begin{vmatrix}
y_1 & \cdots & y_n \\
y_1' & \cdots & y_n' \\
\vdots & & \vdots \\
y_1^{(n-1)} & \cdots & y_n^{(n-1)}
\end{vmatrix}$$

So we can generalise the Wronskian for nth order derivatives! 

\section{Reduction of Order}

Once again, instead of defining, I will aim to start from what we already know and build up from there. 

We already know that a 2nd order ODE has a 2D solution space. And suppose we found one non-trivial (i.e. non-zero) solution $y_1$. And now, we want to find another solution, in particular, a different solution 

We also know that the general solution must look like: $y = c_1 y_1 + c_2 y_2$. But we only have $y_1$, so we can see that we're missing one basis vector. And now this is once again a classic linear algebra problem, given one basis vector, how can we find another that's independent? 

Thinking back to how we worked with Euler type equations, we can let our second solution be the form of: 
$$
y_2(x) = v(x) y_1(x)
$$

This is very similar to change of basis when working with vectors in linear algebra. 

If $v$ were constant, $y_2$ would be a scalar multiple of $y_1$ and hence linearly dependent. Therefore, linear independence is equivalent to requiring $v'(x) \neq 0$.


The Wronskian of $y_1$ and $y_2$ is
$$
W(y_1,y_2) =
\begin{vmatrix}
y_1 & y_2 \\
y_1' & y_2'
\end{vmatrix}
= y_1 y_2' - y_1' y_2.
$$
Substituting $y_2 = v y_1$ and differentiating gives
$$
W(y_1, v y_1) = v'(x)\,y_1(x)^2
$$

Now we can see that the Wronskian is non-zero when $v'(x) \neq 0$, so the new solution introduces a genuinely independent direction in the solution space. 

For any two solutions of the equation $y'' + p(x)y' + q(x)y = 0$, the Wronskian satisfies the first-order differential equation
$$
W'(x) = -p(x)\,W(x)
$$

So we can differentiate the Wronskian and using the original differential equation to eliminate second derivative to get:

$$
W(x) = C e^{-\int p(x)\,dx}
$$

\subsection{Constructing the second solution}

Equating the two expressions for the Wronskian gives
$$
v'(x)\,y_1(x)^2 = C e^{-\int p(x)\,dx},
$$
so that
$$
v'(x) = \frac{C e^{-\int p(x)\,dx}}{y_1(x)^2}
$$
Integrating,
$$
v(x) = \int \frac{e^{-\int p(x)\,dx}}{y_1(x)^2}\,dx
$$
The second solution is therefore:
$$
y_2(x) = y_1(x)\int \frac{e^{-\int p(x)\,dx}}{y_1(x)^2}\,dx
$$


\part{Multivariable Calculus}


