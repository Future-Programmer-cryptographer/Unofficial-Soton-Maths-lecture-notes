\title{MATH1049: Linear Algebra II }
\author{Shub Das \\ sd2g25@soton.ac.uk}
\date{\today}

\maketitle



\tableofcontents 

\chapter{Proofs}

\section{Misconceptions}

This chapter was not something which was covered explicitly in lectures. But I still feel that it is important to learn how to write proofs, in fact, how to write and understand proofs. 

With my experience of linear algebra so far, I found that most confusion in proofs comes from forgetting what objects we're working with. 

Eg: 
\begin{itemize}
    \item Saying \textit{$U$ is a subspace} means checking three specific properties 
    \item Assuming \textit{$x \in Col(A)$} means $x = A u$ for some vector $u$ 
\end{itemize}

The point I'm trying to make is that most proofs begins by assuming the hypotheses and nothing else. So we must avoid assuming what we're trying to prove. If the goal is to show that a set is a subspace, then we cannot assume closure until we have demonstrated it. 

\chapter{Groups}

\section{Introduction}

Before I even define what a group is, \textbf{\href{https://www.southampton.ac.uk/courses/2026-27/modules/math2003}{MATH2003 Group Theory}} is a second year module which goes into a lot more detail about group theory. 

If and when I write `this will be useful later', I will try and briefly mention why. 

\subsection{Cartesian Product}

The Cartesian product is something that comes useful in both algebra and geometry. Eg: when $\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$ is both an algebraic construction (ordered pairs) and a geometric object (plane). Group theory continues this trend by abstracting operations rather than spaces. 

\subsection{Binary operations}

A \textbf{binary operation} on a set $S$ is a function 
$$
\ast : S \times S \rightarrow S 
$$

The key point here is \textbf{closure} - we combine two elements of a set which produces another element in the same set. Closure is another axiom satisfied by groups, which comes under binary operation.

One might wonder why the term binary is used here, and this is simply because the operation reflects the number of inputs- two inputs in, one out. This ensures that other axioms like associativity, identity and inverse made sense.

% how many binary operations + why? 
Given a finite set with $n$ elements, there are $n^{n^2}$ binary operations in that group. 

\subsection{Well-defined operation}

This term has come up before in Number Theory when working with equivalence classes and modular arithmetic. 

An operation is called \textbf{well-defined} if its outputs depends only on the inputs, and not how those inputs are represented. An operation that is not well defined cannot form the basis of a group, since the result of combining two elements would be ambiguous. 

\section{Definitions}

A \textbf{group} is a set $G$ with a binary operation $\ast$ that satisfies the axioms of: 

\begin{itemize}
    \item Associativity: $(a * b ) * c = a * (b * c)$
    \item Identity: $\forall a \in G, \exists e \in G \ \ s.t. \ \ a * e = G$
    \item Inverse: $\forall a \in G,  \exists b \in G \ \ s.t \ \ a * b = e $
\end{itemize}

Associativity is often taken for granted as an axiom, but it's what allows expressions involving many elements to be well-defined. Without this axiom, defining powers becomes problematic. 

\subsection{Uniqueness}

Every inverse is defined relative to the identity. In lecture notes and often in groups, only a right-inverse is assumed: $a \ast b = e$

But using associativity, we can show that this also guarantees a left inverse, and that these two inverses are unique and equal to each other. 

I won't go into detail, but the identity element and inverses in a group are unique. 

\subsection{Identity Elements}

Something that I thought it worth pointing out as mentioned in one of the problem classes, the group axioms don't tell us what the identity is, only wha it is. 

So an element $e$ is the identity if $e \ast a = a \text { and } a \ast e = a , \forall a$ 

So in a group table, the identity row and column reproduce the row and column headers, and there will be exactly one such element. 

\subsection{Abelian Groups}

A group is called \textbf{abelian} if 
$$
a * b = b * a , \ \ \forall a,b \in G 
$$

Notice how this is the same as a group being commutative. However, it's important to note that we should never assume commutativity unless it is explicitly stated. Many familiar numerical groups are abelian, many matrix and symmetry groups are not. 

The sets 
$$
Q^* = Q \setminus \set{0} , \ \ R^* = R \setminus \set{0} 
$$

form an abelian group under multiplication

Zero must be removed from these sets as it has no multiplicative inverses. 

For an abelian group, the group table will be symmetric about the diagonal. 

\subsection{Finite groups}

A group is said to be \textbf{finite} if it has finitely many elements. They often appear in combinatorics, cryptography and puzzles. 

\subsection{Multiplication tables and Sudoku}

In finite groups, we can have multiplication tables that encode the entire group structure. Each row and column must contain every element exactly once, which makes sense due to invertibility. 

\textbf{Note: }If something is a group, then it follows the Sudoku rules, but if it obeys Sudoku rules, it may not necessarily be a group. So Sudoku satisfies the necessary conditions for a group table, but not necessary ones\footnote{The axiom that often gets broken in Sudoku is the associativity one}. 

Other properties such as inverses and exponential laws are summarised well in lecture notes in \textbf{Proposition 1.4}

What I will point out is that the reversal or the order also appears in matrix inverses, function composition and transposing products. 

To think about exponent laws, think of what we're actually doing when it abstract the operation away from the numbers. I.e. we're repeating the same operation repeatedly. So exponents are really laws about repetition in an associative system with identity and inverses, which is by definition of a group! 

\section{Cyclic Groups}

Recall that an \textbf{order} of a group is just the number of elements in that group. And that the \textbf{order of an element} is simply how many times we need to combine the element with itself to get back to the identity for the first time. 

Everything is measured relative to the identity. So if we have a group table, we can spot the identity by looking at the row and column that reproduces the row and column headers. 

The most intuitive way to explain cyclic groups is to think of integers modulo $n$. Group Theory in this way actually comes from Number Theory.

More formally, a group $G$ is cyclic if: 
$$
G = \langle a \rangle = {a^n : n \in \mathbb{Z}}
$$

So every element in the group is obtained by repeatedly applying the operation to a single element. 



\subsection{Group order and elements}

Another interesting thing is that the number of elements in a group doesn't always match the order in that group. Fun example below: 
$$
C_2 \times C_2 \neq C_4
$$

The above can be verified by doing question $4$ from Problem Sheet 2. 

Another way to think about it is that the group $C_4$ is cyclic, i.e. the elements in that group are addition $ \mod 4$. And we if look at the element order, we will have elements of order $4$. 

However, the group $C_2 \times C_2$ has every non-identity element of order $2$. So the group is not cyclic. 

Both groups have element $4$, but their order is different. 

Linking back to Number Theory, we can also see that for a cyclic group $C_n$, the order of an element $k$ is the smallest positive integer $m$ s.t. 
$$
mk \equiv 0 \mod n
$$

So the group itself has order $n$, but not every element has order $n$ 

\section{Symmetries}

When we use the term symmetry of an object, we essentially talk about an action that rearranges the object, but leaves it looking the same. 

Think about rotating a square, reflecting a triangle, or shuffling positions. It is a beautiful thing to notice that symmetries of an object form a group. Why you ask?  
\begin{itemize}
    \item Doing two symmetries in a row is still a symmetry 
    \item Every symmetry can be undone 
    \item There is a `do nothing' symmetry
\end{itemize}

And notice how these are exactly the group axioms! 

\subsection{Symmetry to Permutations}

Instead of thinking about geometry, think about positions: 
$$
\set{ 1, 2, ... , n}
$$

A symmetry now becomes a matter of rearranging items, which we call \textbf{permutation}

So symmetry and permutation groups are basically the same idea, just described differently. 

More formally speaking, a permutation on a set $X$ is a \textbf{bijection} 
$$
\sigma : X \rightarrow X 
$$

Recall we defined what it meant for functions to be a bijection in Calculus I, similarly, a bijection for a group means that: 
\begin{itemize}
    \item injective: no two elements go to the same option 
    \item surjective: every option is achieved 
\end{itemize}

So in other words - nothing is lost, and nothing is duplicated. Bijection is the property that ensures an 
inverse exists and that permutations can be undone. 


\subsection{Composition}

The group operation in permutation groups is composition of functions. So if $\sigma, \tau$ are some permutatoins, then: 
$$
(\sigma \circ \tau )(x) = \sigma(\tau(x))
$$
Notice how this is associative, as function composition always it, but not commutative in general. 

It's also important to know that we apply permutations right to left. 

\subsection{Symmetric groups S_n}

The symmetric group $S_n$ is simply the group of all permutations $\set{1,2,..n}$

The order of this group is (and this comes up often in exam) is: 
$$
| S_n| = n!
$$

\subsection{Notations}

There are two main types of notation: vector and cycle. 

Both notations are explained in the notes, and I can't say any more than what's already written, except to practice doing questions involving joint and disjoint cycles. 

Cycle notation is not unique, so order matters! 

\subsection{Sign of a permutation}

The sign of a permutation tells us whether the permutation preserves the orientation or flips it. Every permutation can be built from swaps of two elements, which we define as a \textbf{transposition}. 

So the question the sign, $sgn(\sigma)$, here is trying to answer is whether we need an odd or an even number of swaps to build the permutation. 

This is defined as: 
$$
sgn(\sigma) = \begin{cases}
    +1 \text{ if $\sigma$ is even} \\
    -1 \text { if $\sigma$ is odd}
\end{cases}
$$

The even and odd here refer to the even and odd number of transpositions. 

This is somewhat like the determinant. If we think about it, swapping two rows of a matrix flips the sign, and the sign also tracks orientations. So $\det$ is the weight sum over permutations, with the sign recording the parity. This will be covered later in the course so do keep an eye out for it!

\subsubsection{Side tangent to permutation ciphers}

It absolutely blows my mind that permutation ciphers are basically permutation groups and all the things discussed above about cycles, inverse, composition show up in this. 

A permutation cipher fixes a block of position, say $n$ letters, then rearranges those position using a permutation $\sigma \in S_n$ 

So if the plaintext block is: 
$$
(x_1, x_2,... x_n)
$$

The encryption is: 
$$
(x_1, x_2,... x_n) \rightarrow (x_{\sigma^{-1}(1)},...x_{\sigma^{-1}(n)}) 
$$

And bijection ensures that these ciphers are decryptable! 


\chapter{Fields and Vector Spaces}

\section{Fields}

A field, defined in this context, is not really the green space we may or may not be able to imagine. Anyway, it's not just a list a of axioms, it's kind of a system in which things are reversible. As a fair warning, linear algebra is going to get really abstract, so I will try my best to explain how things made (or didn't) make sense to me. 

Formally, a \textbf{field} \mathbf{F} is: 
\begin{itemize}
    \item An \textbf{abelian group} under addition 
    \item a multiplicative group on F \setminus \set{0} 
    \item Multiplication distributive over addition
\end{itemize}

When looking at any axioms, it is worth thinking about why they are the way they are. In the case above, each axioms prevents something breaking later. The additive inverses ensures that cancellation works, and multiplicative inverses means that scaling can be undone, and distributivity means scaling respects addition. 

This is also the reason why $\mathbb{Z}$ is not a field, as multiplication cannot always be done. However, things of the form $\mathbb{Z}/\mathbb{Z}_p$ where $p$ is prime is a field, and they are incredibly useful for encryption\footnote{Encryption in a field... something of this reminds of a post on the cipher forum}

\section{Vector spaces}

Recalling the axioms of vector spaces from Linear Algebra I, a vector space is simply a system where: 
\begin{itemize}
    \item Objects can be added together
    \item Objects can be scaled by elements of a field
\end{itemize}

All this time, we're often used to imagining vectors as arrows, and the idea I want to convey here is that these objects don't have to be arrows. They could be functions, polynomials, matrices, sequences, etc. The only thing that matters here is the structure, not objects. 

\subsection{`Vector space over \mathbf{F}'} 

In most linear algebra textbooks, this is a phrase which is used almost all the time. So when we say `$V$ is a vector space over a field $F$', we are fixing which scalars are allowed. This will be very important as changing the field changes what `linear' means, i.e. $\mathbb{R}^n$ over $\mathbb{R}$ and $\mathbb{R}^n$ over $\mathbb{Q}$ are not the same vector space. 

On another note, changing the field doesn't change the vectors themselves, but changes what combination we're allowed to form with them. Eg: if our field was $\mathbb{Q}$, then we cannot scale vectors by an irrational number 

\section{Scalars and vectors}

In the past, we have seen scalars as just real numbers, but now we can generalise scalars to being elements of an arbitrary field $F$. 

The way I'd like to think about it, a scalar is: 
\begin{itemize}
    \item an element of the field $F$ 
    \item something that acts on the vectors
\end{itemize}

Whereas as vector is: 
\begin{itemize}
    \item an element of the vector space $V$
    \item something that gets acted upon
\end{itemize}

This is also why we have two separate distribution laws (look in notes), and why $0_F \neq 0_V$, and $1_F$ exists but there is no $1_V$ 



\chapter{Bases}

\chapter{Linear Transformations}

\chapter{Determinants}

\chapter{Diagonalisability}


